"""
RocketMQ Knowledge Initialization

This module provides built-in RocketMQ knowledge that will be automatically loaded
when the knowledge system is initialized.
"""

import glob
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from loguru import logger

from .store import ChromaKnowledgeStore, DomainKnowledgeManager

# Version control for RocketMQ knowledge
ROCKETMQ_KNOWLEDGE_VERSION = "1.0.0"


def get_rocketmq_content_files(base_path: Path) -> List[Path]:
    """Get list of RocketMQ knowledge content files."""
    knowledge_dir = base_path / "knowledge"
    if not knowledge_dir.exists():
        return []

    # Find all markdown files in knowledge directory
    md_files = []
    for pattern in ["**/*.md", "**/*.MD"]:
        md_files.extend(glob.glob(pattern, recursive=True))

    return md_files


def parse_markdown_file(file_path: Path) -> Dict[str, Any]:
    """Parse markdown file and extract title, content, and metadata."""
    import logging
    logger = logging.getLogger("nanobot.knowledge.rocketmq_init")

    if not file_path.exists():
        logger.warning(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}

    try:
        logger.info(f"ğŸ“– å¼€å§‹è§£æ Markdown æ–‡ä»¶: {file_path.absolute()}")
        content = file_path.read_text(encoding='utf-8')

        # Extract title from first heading
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        title = title_match.group(1).strip() if title_match else file_path.stem

        # Extract tags from file content or path
        tags = []

        # Add category as tag based on directory structure
        parent_dir = file_path.parent.name
        if parent_dir and not parent_dir.startswith('20'):  # Skip date directories
            category_tag = parent_dir.replace('-', ' ').title()
            tags.append(category_tag)
            logger.debug(f"   - æ·»åŠ åˆ†ç±»æ ‡ç­¾: {category_tag}")

        # Add file name keywords as tags
        filename_keywords = re.findall(r'[A-Z][a-z]*|[a-z]+|[A-Z]+', file_path.stem)
        file_tags = [kw.lower() for kw in filename_keywords if len(kw) > 2]
        tags.extend(file_tags)

        if file_tags:
            logger.debug(f"   - æ·»åŠ æ–‡ä»¶åæ ‡ç­¾: {', '.join(file_tags)}")

        # ç»Ÿè®¡å†…å®¹ä¿¡æ¯
        content_length = len(content)
        line_count = content.count('\n') + 1

        logger.info(f"âœ… æ–‡ä»¶è§£ææˆåŠŸ: {title[:30]}...")
        logger.info(f"   - æ–‡ä»¶è·¯å¾„: {file_path.absolute()}")
        logger.info(f"   - æ–‡ä»¶å¤§å°: {content_length} å­—ç¬¦")
        logger.info(f"   - è¡Œæ•°: {line_count}")
        logger.info(f"   - æ ‡ç­¾æ•°: {len(tags)}")
        logger.debug(f"   - æ ‡é¢˜: {title}")
        logger.debug(f"   - å†…å®¹å‰100å­—ç¬¦: {content[:100].replace(chr(10), ' ')}...")

        return {
            "title": title,
            "content": content,
            "tags": tags,
            "file_path": str(file_path.absolute())
        }
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶è§£æå¤±è´¥: {file_path.absolute()}")
        logger.error(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
        return {}


def get_knowledge_categories(base_path: Path, knowledge_dir) -> Dict[str, List[Dict]]:
    """Organize knowledge files by category based on directory structure."""

    knowledge_file_pattern = os.path.join(os.path.expanduser(str(knowledge_dir)), "**", "*.md")

    logger.info(f"ğŸ“‚ æ‰«æçŸ¥è¯†æ–‡ä»¶ç›®å½•...")
    logger.info(f"   - åŸºç¡€è·¯å¾„: {base_path}")
    logger.info(f"   - çŸ¥è¯†ç›®å½•: {knowledge_dir}")
    logger.info(f"   - çŸ¥è¯†æ–‡ä»¶æ ¼å¼: {knowledge_file_pattern}")
    logger.info(f"   - ç›®å½•å­˜åœ¨: {knowledge_dir.exists()}")

    if not knowledge_dir.exists():
        logger.warning("âš ï¸  çŸ¥è¯†ç›®å½•ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºå­—å…¸")
        return {}

    categories = {}
    total_files = 0

    # é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•ä¸­çš„ Markdown æ–‡ä»¶
    logger.info(f"ğŸ” å¼€å§‹é€’å½’æ‰«æçŸ¥è¯†ç›®å½•åŠå…¶æ‰€æœ‰å­ç›®å½•...")

    # ä½¿ç”¨ glob é€’å½’æŸ¥æ‰¾æ‰€æœ‰ Markdown æ–‡ä»¶
    md_files = list(glob.glob(knowledge_file_pattern, recursive=True))
    logger.info(f"ğŸ“„ æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")

    # æŒ‰ç›®å½•ç»“æ„åˆ†ç±»æ–‡ä»¶
    file_groups = {}
    for md_file in md_files:
        file_path = Path(md_file)
        logger.debug(f"ğŸ” å¤„ç†æ–‡ä»¶: {file_path}")

        # è·å–ç›¸å¯¹äºçŸ¥è¯†ç›®å½•çš„ç›¸å¯¹è·¯å¾„æ¥ç¡®å®šåˆ†ç±»
        try:
            relative_path = file_path.relative_to(Path(os.path.expanduser(str(knowledge_dir))))
            category_name = str(relative_path).split('/')[0]
        except ValueError:
            # å¦‚æœæ–‡ä»¶ä¸åœ¨çŸ¥è¯†ç›®å½•ä¸‹ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºåˆ†ç±»
            category_name = file_path.stem

        if category_name not in file_groups:
            file_groups[category_name] = []
        file_groups[category_name].append(file_path)
        logger.debug(f"   - æ–‡ä»¶å·²åˆ†ç±»åˆ°: {category_name}")

    logger.info(f"ğŸ“‚ æŒ‰ç›®å½•ç»“æ„åˆ†ç±»: {len(file_groups)} ä¸ªåˆ†ç±»")
    logger.info(f"   - åˆ†ç±»åˆ—è¡¨: {list(file_groups.keys())}")

    # å¤„ç†æ¯ä¸ªåˆ†ç±»çš„æ–‡ä»¶
    for category_name, files in file_groups.items():
        logger.info(f"ğŸ“‚ å¼€å§‹å¤„ç†åˆ†ç±»: {category_name}")
        logger.info(f"   - æ–‡ä»¶æ•°: {len(files)}")
        logger.info(f"   - æ–‡ä»¶åˆ—è¡¨: {[f.name for f in files]}")

        category_file_count = 0

        for md_file in files:
            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†çŸ¥è¯†æ–‡ä»¶: {md_file.absolute()}")
            knowledge_item = parse_markdown_file(md_file)
            if knowledge_item:
                if category_name not in categories:
                    categories[category_name] = []
                categories[category_name].append(knowledge_item)
                total_files += 1
                category_file_count += 1
                logger.info(f"âœ… æ–‡ä»¶è§£ææˆåŠŸ: {md_file.name} -> {knowledge_item['title'][:30]}...")
                logger.info(f"   - æ–‡ä»¶è·¯å¾„: {md_file.absolute()}")
                logger.info(f"   - æ ‡é¢˜: {knowledge_item['title']}")
                logger.info(f"   - æ ‡ç­¾æ•°: {len(knowledge_item.get('tags', []))}")
            else:
                logger.warning(f"âš ï¸  æ–‡ä»¶è§£æå¤±è´¥: {md_file.absolute()}")

        logger.info(f"âœ… åˆ†ç±» '{category_name}' å¤„ç†å®Œæˆ: {category_file_count}/{len(files)} ä¸ªæ–‡ä»¶æˆåŠŸ")

    logger.info(f"âœ… çŸ¥è¯†æ–‡ä»¶æ‰«æå®Œæˆ:")
    logger.info(f"   - æ‰¾åˆ°åˆ†ç±»æ•°: {len(categories)}")
    logger.info(f"   - æ€»æ–‡ä»¶æ•°: {total_files}")
    logger.info(f"   - å„åˆ†ç±»æ–‡ä»¶æ•°: {', '.join([f'{cat}:{len(items)}' for cat, items in categories.items()])}")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œè®°å½•è­¦å‘Š
    if total_files == 0:
        logger.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„çŸ¥è¯†æ–‡ä»¶")
        logger.warning(f"   - æ‰«æè·¯å¾„: {Path(os.path.expanduser(str(knowledge_dir)))}")
        logger.warning(f"   - æ”¯æŒçš„æ–‡ä»¶ç±»å‹: *.md, *.MD")

    return categories


class RocketMQKnowledgeInitializer:
    """Initializer for built-in RocketMQ knowledge."""

    def __init__(self, knowledge_store):
        """åˆå§‹åŒ– RocketMQ çŸ¥è¯†åˆå§‹åŒ–å™¨.
        
        Args:
            knowledge_store: KnowledgeStore æˆ– ChromaKnowledgeStore å®ä¾‹
        """
        self.store = knowledge_store
        self.domain = "rocketmq"
        self.initialized_count = 0
        self.chunk_count = 0

        # æ£€æµ‹æ˜¯å¦ä¸º ChromaKnowledgeStoreï¼ˆæ”¯æŒå‘é‡åŒ–ï¼‰
        self.is_chroma_store = hasattr(knowledge_store, 'embedder') and hasattr(knowledge_store, 'chunker')

        # å¦‚æœä¸æ˜¯ ChromaKnowledgeStoreï¼Œä½¿ç”¨ DomainKnowledgeManager
        if not self.is_chroma_store:
            self.manager = DomainKnowledgeManager(knowledge_store, self.domain)

        # è·å–åŸºç¡€è·¯å¾„
        if hasattr(knowledge_store, 'workspace'):
            self.base_path = knowledge_store.workspace.parent
        else:
            self.base_path = Path.cwd()

        # åˆå§‹åŒ–æ ‡è®°æ–‡ä»¶è·¯å¾„
        self.init_marker_file = self.base_path / ".rocketmq_init_marker"

    def _is_already_initialized(self) -> bool:
        """æ£€æŸ¥ RocketMQ çŸ¥è¯†åº“æ˜¯å¦å·²ç»åˆå§‹åŒ–è¿‡.
        
        Returns:
            bool: å¦‚æœå·²ç»åˆå§‹åŒ–è¿‡è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """

        # ä½¿ç”¨ store ä¸­çš„ç»Ÿä¸€åˆå§‹åŒ–çŠ¶æ€æ£€æŸ¥æœºåˆ¶
        if hasattr(self.store, '_should_reinitialize'):
            needs_reinit = self.store._should_reinitialize("rocketmq")
            if not needs_reinit:
                logger.info("âœ… RocketMQ çŸ¥è¯†åº“å·²ç»åˆå§‹åŒ–è¿‡ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
                return True
            else:
                logger.info("ğŸ” RocketMQ çŸ¥è¯†åº“éœ€è¦é‡æ–°åˆå§‹åŒ–")
                return False
        else:
            # å¦‚æœ store æ²¡æœ‰ç»Ÿä¸€çŠ¶æ€æ£€æŸ¥æœºåˆ¶ï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘
            logger.warning("âš ï¸  store æ²¡æœ‰ç»Ÿä¸€åˆå§‹åŒ–çŠ¶æ€æ£€æŸ¥æœºåˆ¶ï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘")
            return False

    def force_reinitialize(self):
        """å¼ºåˆ¶é‡æ–°åˆå§‹åŒ– RocketMQ çŸ¥è¯†åº“.
        
        Returns:
            å¦‚æœæ˜¯ ChromaKnowledgeStore: (item_count, chunk_count)
            å¦‚æœæ˜¯å…¶ä»–å­˜å‚¨ç±»å‹: item_count
        """
        import logging
        logger = logging.getLogger("nanobot.knowledge.rocketmq_init")

        logger.warning("ğŸ”„ å¼ºåˆ¶é‡æ–°åˆå§‹åŒ– RocketMQ çŸ¥è¯†åº“")

        # å¼ºåˆ¶é‡æ–°åˆå§‹åŒ– RocketMQ çŸ¥è¯†åº“
        if hasattr(self.store, '_init_status') and 'rocketmq' in self.store._init_status:
            del self.store._init_status['rocketmq']
            self.store._save_init_status()
            logger.info("ğŸ—‘ï¸ å·²æ¸…é™¤ RocketMQ çŸ¥è¯†åº“çš„åˆå§‹åŒ–çŠ¶æ€")

        # æ‰§è¡Œåˆå§‹åŒ–
        return self.initialize()

    def initialize(self):
        """Initialize built-in RocketMQ knowledge from file system.
        
        Returns:
            å¦‚æœæ˜¯ ChromaKnowledgeStore: (item_count, chunk_count)
            å¦‚æœæ˜¯å…¶ä»–å­˜å‚¨ç±»å‹: item_count
        """

        # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå§‹åŒ–è¿‡
        if self._is_already_initialized():
            logger.info("ğŸš€ RocketMQ çŸ¥è¯†åº“å·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡æœ¬æ¬¡åˆå§‹åŒ–")
            return (0, 0) if self.is_chroma_store else 0

        logger.info(f"ğŸš€ å¼€å§‹åˆå§‹åŒ– RocketMQ çŸ¥è¯†åº“")
        logger.info(f"   - å­˜å‚¨ç±»å‹: ChromaKnowledgeStore (å‘é‡åŒ–)")
        logger.info(f"   - åŸºç¡€è·¯å¾„: {self.base_path}")

        self.initialized_count = 0
        self.chunk_count = 0

        # Load knowledge from file system
        logger.info("ğŸ“‚ æ­£åœ¨åŠ è½½çŸ¥è¯†æ–‡ä»¶...")
        categories = get_knowledge_categories(self.base_path, self.store.knowledge_dir)

        logger.info(
            f"âœ… æ‰¾åˆ° {len(categories)} ä¸ªçŸ¥è¯†ç±»åˆ«ï¼Œå…± {sum(len(items) for items in categories.values())} ä¸ªçŸ¥è¯†æ¡ç›®")
        # Initialize from file system
        self._initialize_from_filesystem(categories)

        # åˆå§‹åŒ–çŠ¶æ€ç”± store ç»Ÿä¸€ç®¡ç†ï¼Œæ— éœ€å•ç‹¬åˆ›å»ºæ ‡è®°æ–‡ä»¶
        logger.info("âœ… RocketMQ çŸ¥è¯†åº“åˆå§‹åŒ–çŠ¶æ€å·²ç”± store ç»Ÿä¸€ç®¡ç†")

        logger.info(f"ğŸ‰ RocketMQ çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"ğŸ“Š åˆå§‹åŒ–ç»“æœç»Ÿè®¡:")
        logger.info(f"   - å­˜å‚¨ç±»å‹: ChromaKnowledgeStore (å‘é‡åŒ–)")
        logger.info(f"   - åˆå§‹åŒ–çŸ¥è¯†æ¡ç›®æ•°: {self.initialized_count}")
        if self.is_chroma_store:
            logger.info(f"   - å‘é‡åŒ–æ–‡æœ¬å—æ•°: {self.chunk_count}")
            logger.info(
                f"   - å¹³å‡æ¯ä¸ªæ¡ç›®åˆ†å—æ•°: {self.chunk_count / self.initialized_count if self.initialized_count > 0 else 0:.1f}")
        logger.info(f"   - åŸºç¡€è·¯å¾„: {self.base_path}")
        logger.info(f"   - çŸ¥è¯†åŸŸ: {self.domain}")
        logger.info(f"   - åˆå§‹åŒ–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        if self.initialized_count == 0:
            logger.warning("âš ï¸  è­¦å‘Š: æœªæˆåŠŸåˆå§‹åŒ–ä»»ä½•çŸ¥è¯†æ¡ç›®")
        else:
            logger.info("âœ… çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨çŸ¥è¯†æœç´¢åŠŸèƒ½")

        # æ›´æ–° store çš„åˆå§‹åŒ–çŠ¶æ€
        if hasattr(self.store, '_init_status'):
            self.store._init_status["rocketmq"] = {
                "initialized_at": datetime.now().isoformat(),
                "item_count": self.initialized_count,
                "chunk_count": self.chunk_count if self.is_chroma_store else 0,
                "last_check": datetime.now().isoformat(),
            }
            logger.info("âœ… å·²æ›´æ–° store çš„åˆå§‹åŒ–çŠ¶æ€")
            logger.info(f"   - item_count: {self.initialized_count}")
            logger.info(f"   - chunk_count: {self.chunk_count if self.is_chroma_store else 0}")

        self.store._save_init_status()

        if self.is_chroma_store:
            return self.initialized_count, self.chunk_count
        else:
            return self.initialized_count



    def _increment_count(self) -> None:
        """Increment the initialization counter."""
        self.initialized_count += 1

    def _initialize_from_filesystem(self, categories: Dict[str, List[Dict]]) -> None:
        """Initialize knowledge from file system categories."""
        import logging
        logger = logging.getLogger("nanobot.knowledge.rocketmq_init")

        logger.info("ğŸ“ å¼€å§‹å¤„ç†çŸ¥è¯†æ–‡ä»¶...")

        for category_name, knowledge_items in categories.items():
            logger.info(f"ğŸ“ å¤„ç†ç±»åˆ« '{category_name}': {len(knowledge_items)} ä¸ªæ¡ç›®")

            for i, item in enumerate(knowledge_items, 1):
                # Determine knowledge type based on category and content
                knowledge_type = self._determine_knowledge_type(category_name, item["content"])

                logger.info(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†æ¡ç›® {i}/{len(knowledge_items)}: {item['title'][:50]}...")
                logger.debug(f"   - çŸ¥è¯†ç±»å‹: {knowledge_type}")
                logger.debug(f"   - æ–‡ä»¶æ¥æº: {item.get('file_path', 'æœªçŸ¥')}")

                try:
                    if self.is_chroma_store:
                        # ä½¿ç”¨å‘é‡åŒ–å­˜å‚¨
                        self._add_knowledge_with_vectorization(
                            knowledge_type=knowledge_type,
                            title=item["title"],
                            content=item["content"],
                            tags=item["tags"]
                        )
                    else:
                        # ä½¿ç”¨æ—§çš„å­˜å‚¨æ–¹å¼
                        if knowledge_type == "troubleshooting":
                            self.manager.add_troubleshooting_guide(
                                title=item["title"],
                                content=item["content"],
                                tags=item["tags"]
                            )
                        elif knowledge_type == "configuration":
                            self.manager.add_configuration_guide(
                                title=item["title"],
                                content=item["content"],
                                tags=item["tags"]
                            )
                        elif knowledge_type == "best_practice":
                            self.manager.add_best_practice(
                                title=item["title"],
                                content=item["content"],
                                tags=item["tags"]
                            )
                        else:
                            # Default to troubleshooting guide
                            self.manager.add_troubleshooting_guide(
                                title=item["title"],
                                content=item["content"],
                                tags=item["tags"]
                            )

                    self._increment_count()
                    logger.info(f"âœ… çŸ¥è¯†æ¡ç›®åˆå§‹åŒ–æˆåŠŸ: {item['title'][:30]}...")

                except Exception as e:
                    logger.error(f"âŒ çŸ¥è¯†æ¡ç›®åˆå§‹åŒ–å¤±è´¥: {item['title'][:30]}...")
                    logger.error(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
                    logger.error(f"   æ–‡ä»¶è·¯å¾„: {item.get('file_path', 'æœªçŸ¥')}")

            logger.info(f"âœ… ç±»åˆ« '{category_name}' å¤„ç†å®Œæˆ: {len(knowledge_items)} ä¸ªæ¡ç›®")

        logger.info(f"âœ… æ‰€æœ‰çŸ¥è¯†æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…± {self.initialized_count} ä¸ªæ¡ç›®")

    def _add_knowledge_with_vectorization(
            self,
            knowledge_type: str,
            title: str,
            content: str,
            tags: List[str]
    ) -> None:
        """æ·»åŠ çŸ¥è¯†å¹¶è¿›è¡Œå‘é‡åŒ–å­˜å‚¨.
        
        Args:
            knowledge_type: çŸ¥è¯†ç±»å‹ï¼ˆtroubleshooting, configuration, best_practiceï¼‰
            title: æ ‡é¢˜
            content: å†…å®¹
            tags: æ ‡ç­¾åˆ—è¡¨
        """
        from datetime import datetime

        # ç”Ÿæˆå”¯ä¸€ ID
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S%f")
        item_id = f"{self.domain}_{timestamp}"

        # ç¡®å®šä¼˜å…ˆçº§
        priority_map = {
            "troubleshooting": 3,
            "configuration": 2,
            "best_practice": 4
        }
        priority = priority_map.get(knowledge_type, 2)

        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            "item_id": item_id,
            "domain": self.domain,
            "category": knowledge_type,
            "title": title,
            "tags": tags,
            "source": "system",
            "priority": priority,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }

        try:
            logger.info(f"ğŸ§© å¼€å§‹å‘é‡åŒ–å¤„ç†çŸ¥è¯†æ¡ç›®: {title[:50]}...")
            logger.debug(f"   - æ¡ç›®ID: {item_id}")
            logger.debug(f"   - çŸ¥è¯†ç±»å‹: {knowledge_type}")
            logger.debug(f"   - å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

            # 1. æ–‡æœ¬åˆ†å—
            logger.info(f"ğŸ“„ æ­£åœ¨å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—å¤„ç†...")
            chunks = self.store.chunker.chunk_text(content, metadata)

            if not chunks:
                logger.warning(f"âš ï¸ çŸ¥è¯†æ¡ç›® {item_id} åˆ†å—åä¸ºç©ºï¼Œè·³è¿‡")
                return

            logger.info(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªåˆ†å—")
            logger.debug(f"   - å¹³å‡åˆ†å—å¤§å°: {sum(len(chunk['text']) for chunk in chunks) / len(chunks):.0f} å­—ç¬¦")

            # 2. æ‰¹é‡å‘é‡åŒ–
            logger.info(f"ğŸ”¢ æ­£åœ¨å¯¹ {len(chunks)} ä¸ªåˆ†å—è¿›è¡Œå‘é‡åŒ–...")
            chunk_texts = [chunk["text"] for chunk in chunks]
            try:
                embeddings = self.store.embedder.embed_batch(chunk_texts)
                logger.info(f"âœ… å‘é‡åŒ–å®Œæˆ: {len(embeddings)} ä¸ªå‘é‡ï¼Œç»´åº¦: {len(embeddings[0]) if embeddings else 0}")
                logger.debug(f"   - å‘é‡åŒ–æˆåŠŸç‡: {len(embeddings)}/{len(chunks)}")
            except Exception as e:
                logger.error(f"âŒ çŸ¥è¯†æ¡ç›® {item_id} å‘é‡åŒ–å¤±è´¥: {str(e)}")
                return

            # 3. å­˜å‚¨åˆ° Chroma
            logger.info(f"ğŸ’¾ æ­£åœ¨å­˜å‚¨åˆ° Chroma æ•°æ®åº“...")
            collection = self.store._get_or_create_collection(self.domain)

            # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®
            ids = []
            documents = []
            metadatas = []
            embeddings_list = []

            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                chunk_id = f"{item_id}_chunk_{i}"
                ids.append(chunk_id)
                documents.append(chunk["text"])
                metadatas.append(chunk["metadata"])
                embeddings_list.append(embedding)

            # æ‰¹é‡æ’å…¥åˆ° Chroma
            collection.add(
                ids=ids,
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings_list
            )

            # æ›´æ–°åˆ†å—è®¡æ•°
            self.chunk_count += len(chunks)

            logger.info(f"âœ… çŸ¥è¯†æ¡ç›® '{title[:30]}...' å·²æˆåŠŸå­˜å‚¨: {len(chunks)} ä¸ªåˆ†å—")
            logger.info(f"ğŸ“Š å­˜å‚¨ç»Ÿè®¡: æ¡ç›® {self.initialized_count + 1}, åˆ†å— {self.chunk_count}")
            logger.debug(f"   - å­˜å‚¨é›†åˆ: {self.domain}")
            logger.debug(f"   - å­˜å‚¨æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")

        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†æ¡ç›® {item_id} å­˜å‚¨å¤±è´¥: {str(e)}")
            logger.error(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
            logger.error(f"   æ¡ç›®æ ‡é¢˜: {title}")
            logger.error(f"   çŸ¥è¯†ç±»å‹: {knowledge_type}")

    def _determine_knowledge_type(self, category_name: str, content: str) -> str:
        """Determine the type of knowledge based on category and content."""
        category_lower = category_name.lower()
        content_lower = content.lower()

        if any(keyword in category_lower for keyword in ['troubleshoot', 'problem', 'issue', 'error', 'æ•…éšœ', 'é—®é¢˜']):
            return "troubleshooting"
        elif any(keyword in category_lower for keyword in ['config', 'setup', 'install', 'éƒ¨ç½²', 'é…ç½®', 'å®‰è£…']):
            return "configuration"
        elif any(keyword in category_lower for keyword in ['best', 'practice', 'guide', 'æœ€ä½³', 'å®è·µ', 'æŒ‡å—']):
            return "best_practice"
        elif any(keyword in content_lower for keyword in ['æ’æŸ¥', 'é—®é¢˜', 'é”™è¯¯', 'æ•…éšœ', 'troubleshoot', 'problem']):
            return "troubleshooting"
        elif any(keyword in content_lower for keyword in ['é…ç½®', 'å®‰è£…', 'éƒ¨ç½²', 'config', 'setup', 'install']):
            return "configuration"
        elif any(keyword in content_lower for keyword in ['æœ€ä½³', 'å®è·µ', 'æŒ‡å—', 'best', 'practice', 'guide']):
            return "best_practice"

        return "troubleshooting"  # Default type


def initialize_rocketmq_knowledge(workspace: Path) -> int | tuple[int, int]:
    """Initialize built-in RocketMQ knowledge."""
    from nanobot.knowledge.rag_config import RAGConfig
    from nanobot.config.loader import load_config
    
    # åˆ›å»º RAGConfig å¹¶ä»é…ç½®æ–‡ä»¶åŠ è½½ rerank è®¾ç½®
    rag_config = RAGConfig.from_env()
    
    # å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰reranké…ç½®ï¼Œå°è¯•ä»config.jsonåŠ è½½
    try:
        config = load_config()
        if config.rerank.model_path:
            rag_config.rerank_model_path = config.rerank.model_path
        if config.rerank.threshold > 0:
            rag_config.rerank_threshold = config.rerank.threshold
    except Exception:
        pass  # ä½¿ç”¨é»˜è®¤é…ç½®
    
    store = ChromaKnowledgeStore(workspace, rag_config)
    initializer = RocketMQKnowledgeInitializer(store)
    return initializer.initialize()
