"""Text chunking for long documents."""

import logging
from typing import Any, Dict, List

from langchain_text_splitters import RecursiveCharacterTextSplitter
from loguru import logger

class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨ï¼Œå°†é•¿æ–‡æœ¬åˆ†å‰²ä¸ºè¯­ä¹‰å—."""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200, 
                 smart_chunking: bool = False, preserve_structure: bool = False):
        """åˆå§‹åŒ–åˆ†å—å™¨.
        
        Args:
            chunk_size: å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: å—é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            smart_chunking: å¯ç”¨æ™ºèƒ½åˆ†å‰²ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
            preserve_structure: ä¿æŒæ–‡æ¡£ç»“æ„ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.smart_chunking = False  # å¼ºåˆ¶å…³é—­æ™ºèƒ½åˆ†å—
        self.preserve_structure = False  # å¼ºåˆ¶å…³é—­ç»“æ„ä¿æŒ
        self.splitter = None
        self._init_splitter()

    def _init_splitter(self) -> None:
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ï¼Œé…ç½®ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦."""
        # ç»Ÿä¸€ä½¿ç”¨æ ‡å‡†åˆ†å‰²ï¼Œèšç„¦åˆ†éš”ç¬¦åˆ†å‰²
        separators = [
            "CHUNK_BOUNDARY",  # æ‰‹åŠ¨åˆ†å—æ ‡è®°ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            "\n\n```",  # ä»£ç å—
            "\n\n",  # æ®µè½åˆ†éš”ç¬¦
            "\n```",  # ä»£ç å—ï¼ˆæ— å‰å¯¼æ¢è¡Œï¼‰
            "ã€‚",  # ä¸­æ–‡å¥å·
            "ï¼",  # ä¸­æ–‡æ„Ÿå¹å·
            "ï¼Ÿ",  # ä¸­æ–‡é—®å·
            "ï¼›",  # ä¸­æ–‡åˆ†å·
            ".",  # è‹±æ–‡å¥å·
            "!",  # è‹±æ–‡æ„Ÿå¹å·
            "?",  # è‹±æ–‡é—®å·
            ";",  # è‹±æ–‡åˆ†å·
            "ï¼Œ",  # ä¸­æ–‡é€—å·
            ",",  # è‹±æ–‡é€—å·
            " ",  # ç©ºæ ¼
            "",  # å­—ç¬¦çº§åˆ«åˆ†å‰²
        ]
            
        # ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å‰²ç­–ç•¥ï¼Œä¼˜å…ˆåœ¨æ®µè½ã€å¥å­è¾¹ç•Œå¤„åˆ†å—
        # åˆ†éš”ç¬¦æŒ‰ä¼˜å…ˆçº§æ’åºï¼šæ‰‹åŠ¨æ ‡è®° > ä»£ç å— > æ®µè½ > å¥å­ > æ ‡ç‚¹ > ç©ºæ ¼ > å­—ç¬¦
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=separators,
            keep_separator=True,  # ä¿ç•™åˆ†éš”ç¬¦
            length_function=len,
        )
        
        logger.info(f"åˆ†éš”ç¬¦åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ: chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
        logger.info(f"åˆ†éš”ç¬¦ä¼˜å…ˆçº§: CHUNK_BOUNDARY > ä»£ç å— > æ®µè½ > å¥å­ > æ ‡ç‚¹")

    def chunk_text(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†å—æ–‡æœ¬å¹¶ä¿ç•™å…ƒæ•°æ®.
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            metadata: å…ƒæ•°æ®ï¼ˆid, domain, category, title, tags ç­‰ï¼‰
            
        Returns:
            åˆ†å—ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« text å’Œ metadata
            å¦‚æœæ–‡æœ¬é•¿åº¦ä¸è¶…è¿‡ chunk_sizeï¼Œè¿”å›å•ä¸ªå—
        """
        if not text or not text.strip():
            logger.warning("å°è¯•åˆ†å—ç©ºæ–‡æœ¬ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        # æ£€æµ‹æ˜¯å¦åŒ…å«æ‰‹åŠ¨åˆ†å—æ ‡è®°
        chunk_markers = ["CHUNK_BOUNDARY"]
        has_chunk_marker = any(marker in text for marker in chunk_markers)
        
        if has_chunk_marker:
            logger.info(f"æ£€æµ‹åˆ°æ‰‹åŠ¨åˆ†å—æ ‡è®°ï¼Œæ–‡æ¡£: {metadata.get('title', 'Unknown')}")
            for marker in chunk_markers:
                if marker in text:
                    chunk_count = text.count(marker)
                    logger.info(f"{marker} æ ‡è®°æ•°é‡: {chunk_count}")
                    break

        # å¦‚æœæ–‡æœ¬é•¿åº¦ä¸è¶…è¿‡ chunk_sizeï¼Œä¸éœ€è¦åˆ†å—
        if len(text) <= self.chunk_size:
            logger.debug(f"æ–‡æœ¬é•¿åº¦ {len(text)} ä¸è¶…è¿‡ chunk_size {self.chunk_size}ï¼Œä¸åˆ†å—")
            return [{
                "text": text,
                "metadata": {
                    **metadata,
                    "chunk_index": 0,
                    "total_chunks": 1,
                }
            }]

        # ä½¿ç”¨æ ‡å‡†åˆ†å—ï¼Œä¸å†ä½¿ç”¨æ™ºèƒ½åˆ†å—
        try:
            chunks = self.splitter.split_text(text)
                
            logger.info(f"æ–‡æœ¬åˆ†å—å®Œæˆ: åŸå§‹é•¿åº¦={len(text)}, åˆ†å—æ•°={len(chunks)}")

            # æ‰“å°åŸå§‹åˆ†å—çš„è¯¦ç»†ä¿¡æ¯
            logger.debug("=== åŸå§‹åˆ†å—è¯¦æƒ… ===")
            for i, chunk in enumerate(chunks):
                chunk_preview = chunk.replace('\n', '\\n')[:100]
                logger.debug(f"åŸå§‹Chunk {i+1}: é•¿åº¦={len(chunk)}, é¢„è§ˆ='{chunk_preview}...'")
                for marker in chunk_markers:
                    if marker in chunk:
                        logger.debug(f"  âš ï¸ Chunk {i+1} åŒ…å«{marker}æ ‡è®°")
                        break

            # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ å…ƒæ•°æ®
            result = []
            for i, chunk in enumerate(chunks):
                # å¦‚æœchunkåŒ…å«CHUNK_BOUNDARYæ ‡è®°ï¼Œå³ä½¿å†…å®¹è¾ƒå°‘ä¹Ÿä¿ç•™
                has_marker = any(marker in chunk for marker in chunk_markers)
                if has_marker:
                    result.append({
                        "text": chunk,
                        "metadata": {
                            **metadata,
                            "chunk_index": len(result),
                            "total_chunks": len(chunks),
                        }
                    })
                    continue
                    
                # å¯¹äºä¸åŒ…å«CHUNK_BOUNDARYæ ‡è®°çš„chunkï¼Œè¿‡æ»¤æ‰å†…å®¹å¤ªå°‘çš„
                clean_chunk = chunk
                for marker in chunk_markers:
                    clean_chunk = clean_chunk.replace(marker, "")
                clean_chunk = clean_chunk.strip()
                
                if len(clean_chunk) < 10:  # è¿‡æ»¤æ‰å†…å®¹å¤ªå°‘çš„chunk
                    logger.debug(f"è·³è¿‡å†…å®¹è¿‡å°‘çš„chunk {i+1}: {len(clean_chunk)} å­—ç¬¦, å†…å®¹='{clean_chunk}'")
                    continue
                    
                result.append({
                    "text": chunk,
                    "metadata": {
                        **metadata,
                        "chunk_index": len(result),
                        "total_chunks": len(chunks),
                    }
                })

            # æ›´æ–°æ€»chunkæ•°é‡
            for item in result:
                item["metadata"]["total_chunks"] = len(result)

            # æ‰“å°æœ€ç»ˆåˆ†å—çš„è¯¦ç»†ä¿¡æ¯
            logger.debug("=== æœ€ç»ˆåˆ†å—è¯¦æƒ… ===")
            for i, item in enumerate(result):
                chunk = item["text"]
                chunk_preview = chunk.replace('\n', '\\n')
                has_boundary = any(marker in chunk for marker in chunk_markers)
                logger.debug(f"æœ€ç»ˆChunk {i+1}: é•¿åº¦={len(chunk)}, BOUNDARY={has_boundary}")
                logger.debug(f"  å…¨éƒ¨chunkå†…å®¹: {chunk_preview}")
                
                # å¦‚æœchunkåŒ…å«æ ‡é¢˜ï¼Œç‰¹åˆ«æ ‡æ³¨
                if any(marker in chunk for marker in ["####", "###", "**æ­¥éª¤"]):
                    titles = []
                    if "####" in chunk:
                        titles.append("å››çº§æ ‡é¢˜")
                    if "###" in chunk:
                        titles.append("ä¸‰çº§æ ‡é¢˜")
                    if "**æ­¥éª¤" in chunk:
                        titles.append("æ­¥éª¤æ ‡è®°")
                    logger.debug(f"  ğŸ“‹ åŒ…å«ç»“æ„: {', '.join(titles)}")

            logger.info(f"è¿‡æ»¤åçš„åˆ†å—æ•°é‡: {len(result)} (åŸå§‹: {len(chunks)})")
            return result

        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ†å—å¤±è´¥: {str(e)}", exc_info=True)
            # åˆ†å—å¤±è´¥æ—¶ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬ä½œä¸ºå•ä¸ªå—
            logger.warning("åˆ†å—å¤±è´¥ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬ä½œä¸ºå•ä¸ªå—")
            return [{
                "text": text,
                "metadata": {
                    **metadata,
                    "chunk_index": 0,
                    "total_chunks": 1,
                }
            }]