"""Web interface for nanobot."""

import os
from pathlib import Path
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse


class ConnectionManager:
    """Manage WebSocket connections."""

    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)


# Create FastAPI application
web_app = FastAPI(
    title="nanobot Web UI",
    description="Web interface for nanobot"
)

# Create connection manager instance
manager = ConnectionManager()


def load_html_template(template_name: str) -> str:
    """Load HTML template from file."""
    template_path = Path(__file__).parent / "templates" / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return f"<html><body><h1>Template not found: {template_name}</h1></body></html>"
    except Exception as e:
        return f"<html><body><h1>Error loading template: {str(e)}</h1></body></html>"


@web_app.get("/")
async def get():
    """Serve the Web UI homepage."""
    html_content = load_html_template("index.html")
    return HTMLResponse(content=html_content)


@web_app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """Handle WebSocket connections with real-time streaming."""
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # Process user message with real-time streaming
            await process_user_message_streaming(data, websocket)
    except WebSocketDisconnect:
        manager.disconnect(websocket)


async def process_user_message_streaming(user_input: str, websocket: WebSocket):
    """Process user message with real-time streaming output."""
    import time
    import json
    from nanobot.config.loader import load_config
    from nanobot.bus.queue import MessageBus
    from nanobot.agent.loop import AgentLoop

    start_time = time.time()

    config = load_config()
    bus = MessageBus()

    # Create provider from config
    from nanobot.providers.litellm_provider import LiteLLMProvider
    p = config.get_provider()
    model = config.agents.defaults.model
    if not (p and p.api_key) and not model.startswith("bedrock/"):
        await websocket.send_text("Error: No API key configured. Please set one in ~/.nanobot/config.json")
        return

    provider = LiteLLMProvider(
        api_key=p.api_key if p else None,
        api_base=config.get_api_base(),
        default_model=model,
        extra_headers=p.extra_headers if p else None,
        provider_name=config.get_provider_name(),
    )

    agent_loop = AgentLoop(
        bus=bus,
        provider=provider,
        workspace=config.workspace_path,
        brave_api_key=config.tools.web.search.api_key or None,
        exec_config=config.tools.exec,
        restrict_to_workspace=config.tools.restrict_to_workspace,
    )

    # Send initial processing message
    await websocket.send_text("ðŸ¤– AI Agent is processing your request...\\n\\n")

    # Record LLM start time
    llm_start_time = time.time()

    # è®¾ç½®æµå¼å›žè°ƒå‡½æ•°
    async def stream_callback(context_info: dict):
        """æµå¼è¾“å‡ºå›žè°ƒå‡½æ•°ï¼ŒæŒ‰ç±»åž‹åˆ†ç±»æ˜¾ç¤ºå†…å®¹ï¼Œå¹¶ç»Ÿè®¡æ¯æ¬¡è¿”å›žçš„è€—æ—¶"""
        content = context_info.get('content', '')
        if not content.strip():
            return

        # è®°å½•å½“å‰å›žè°ƒçš„æ—¶é—´
        callback_time = time.time()

        # æ ¹æ®å†…å®¹ç±»åž‹æ·»åŠ åˆ†ç±»æ ‡è®°
        content_type = 'reasoning'
        if context_info.get('is_final_answer', False):
            content_type = 'answer'
        elif context_info.get('is_tool_call', False):
            content_type = 'tool'
        elif context_info.get('is_iteration_start', False):
            content_type = 'iteration'

        # è®¡ç®—ä»Žå¼€å§‹å¤„ç†åˆ°å½“å‰å›žè°ƒçš„è€—æ—¶
        current_duration = round(callback_time - start_time, 3)

        # èŽ·å–è¿­ä»£è®¡æ•°ä¿¡æ¯
        iteration_count = context_info.get('iteration_count', 0)

        # ä¸ºä¸åŒç±»åž‹çš„å†…å®¹æ·»åŠ é€‚å½“çš„æ ‡è®°ï¼Œé¿å…é‡å¤ä¿¡æ¯
        if content_type == 'iteration':
            # è¿­ä»£å¼€å§‹ä¿¡æ¯
            enhanced_content = f"ðŸ”„ ç¬¬{iteration_count}æ¬¡è¿­ä»£å¼€å§‹\\n"
        elif content_type == 'tool':
            # å·¥å…·æ‰§è¡Œä¿¡æ¯ - åªæ·»åŠ çŠ¶æ€æ ‡è®°ï¼Œä¸é‡å¤æ·»åŠ è€—æ—¶ä¿¡æ¯
            tool_status = context_info.get('tool_status', '')
            if tool_status == 'start':
                enhanced_content = f"ðŸ”§ å¼€å§‹æ‰§è¡Œå·¥å…·\\n{content}"
            elif tool_status == 'completed':
                enhanced_content = f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆ\\n{content}"
            elif tool_status == 'error':
                enhanced_content = f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥\\n{content}"
            else:
                enhanced_content = f"ðŸ”§ å·¥å…·æ‰§è¡Œ\\n{content}"
        else:
            # å…¶ä»–ç±»åž‹å†…å®¹ - ç›´æŽ¥ä½¿ç”¨åŽŸå§‹å†…å®¹ï¼Œä¸æ·»åŠ é¢å¤–ä¿¡æ¯
            enhanced_content = content

        # å‘é€å¸¦ç±»åž‹æ ‡è®°å’Œè€—æ—¶ç»Ÿè®¡çš„å†…å®¹
        message_data = {
            'type': 'stream_chunk',
            'content_type': content_type,
            'content': enhanced_content,
            'is_reasoning': context_info.get('is_reasoning', False),
            'is_tool_call': content_type == 'tool' or context_info.get('is_tool_call', False),
            'is_final_answer': context_info.get('is_final_answer', False),
            'is_iteration_start': context_info.get('is_iteration_start', False),
            'timestamp': callback_time,
            'duration_from_start': current_duration,
            'iteration_count': iteration_count,
        }

        # å¦‚æžœæ˜¯å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ å·¥å…·åç§°å’ŒçŠ¶æ€ä¿¡æ¯
        if content_type == 'tool':
            message_data['tool_name'] = context_info.get('tool_name', '')
            message_data['tool_status'] = context_info.get('tool_status', '')
            message_data['tool_duration'] = context_info.get('tool_duration', 0)
            message_data['tool_result'] = context_info.get('tool_result', '')
            message_data['tool_error'] = context_info.get('tool_error', '')
            message_data['tool_args'] = context_info.get('tool_args')

        await websocket.send_text(json.dumps(message_data, ensure_ascii=False))

    # ä¸ºagent_loopè®¾ç½®æµå¼å›žè°ƒ
    agent_loop.stream_callback = stream_callback

    # Process with streaming output
    response = await agent_loop.process_direct(user_input, session_key="cli:webui")

    # Record LLM end time
    llm_end_time = time.time()
    llm_execution_time = round(llm_end_time - llm_start_time, 1)

    # Send the actual response (å¦‚æžœæµå¼è¾“å‡ºå·²ç»å‘é€äº†å†…å®¹ï¼Œè¿™é‡Œå¯èƒ½ä¸éœ€è¦å†å‘é€)
    if response and response.strip():
        # æ£€æŸ¥æ˜¯å¦å·²ç»é€šè¿‡æµå¼è¾“å‡ºå‘é€äº†å†…å®¹
        # å¦‚æžœæ²¡æœ‰æµå¼è¾“å‡ºï¼Œåˆ™å‘é€å®Œæ•´å“åº”
        await websocket.send_text("\\n" + response)
    elif not response:
        await websocket.send_text("No response from agent.")

    end_time = time.time()
    total_processing_time = round(end_time - start_time, 1)

    # Send processing times
    await websocket.send_text(f"\\n---\\n*æ€»è€—æ—¶: {total_processing_time}ç§’ | LLMæ‰§è¡Œè€—æ—¶: {llm_execution_time}ç§’*")


async def process_user_message(user_input: str) -> str:
    """Process user message using nanobot's AgentLoop."""
    import time
    from nanobot.config.loader import load_config
    from nanobot.bus.queue import MessageBus
    from nanobot.agent.loop import AgentLoop

    start_time = time.time()

    config = load_config()
    bus = MessageBus()

    # Create provider from config
    from nanobot.providers.litellm_provider import LiteLLMProvider
    p = config.get_provider()
    model = config.agents.defaults.model
    if not (p and p.api_key) and not model.startswith("bedrock/"):
        return "Error: No API key configured. Please set one in ~/.nanobot/config.json"

    provider = LiteLLMProvider(
        api_key=p.api_key if p else None,
        api_base=config.get_api_base(),
        default_model=model,
        extra_headers=p.extra_headers if p else None,
        provider_name=config.get_provider_name(),
    )

    agent_loop = AgentLoop(
        bus=bus,
        provider=provider,
        workspace=config.workspace_path,
        brave_api_key=config.tools.web.search.api_key or None,
        exec_config=config.tools.exec,
        restrict_to_workspace=config.tools.restrict_to_workspace,
    )

    # Record LLM start time
    llm_start_time = time.time()

    response = await agent_loop.process_direct(user_input, session_key="cli:webui")

    # Record LLM end time
    llm_end_time = time.time()
    llm_execution_time = round(llm_end_time - llm_start_time, 1)

    end_time = time.time()
    total_processing_time = round(end_time - start_time, 1)

    if response:
        return f"{response}\\n\\n---\\n*æ€»è€—æ—¶: {total_processing_time}ç§’ | LLMæ‰§è¡Œè€—æ—¶: {llm_execution_time}ç§’*"
    else:
        return f"No response from agent.\\n\\n---\\n*æ€»è€—æ—¶: {total_processing_time}ç§’ | LLMæ‰§è¡Œè€—æ—¶: {llm_execution_time}ç§’*"
