"""Web interface for nanobot with intent classification."""

from pathlib import Path

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from loguru import logger

from nanobot.agent import AgentLoop
from nanobot.config import load_config, Config
from nanobot.providers import LLMProvider


def diagnose_knowledge_base(workspace_path: Path) -> dict:
    """è¯Šæ–­çŸ¥è¯†åº“çŠ¶æ€."""
    try:
        from nanobot.knowledge.store import ChromaKnowledgeStore

        # æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•
        knowledge_dir = workspace_path / "knowledge"
        chroma_dir = knowledge_dir / "chroma_db"

        status = {
            "available": False,
            "knowledge_dir_exists": knowledge_dir.exists(),
            "chroma_dir_exists": chroma_dir.exists(),
            "total_collections": 0,
            "total_documents": 0,
            "error": None
        }

        if not knowledge_dir.exists():
            status["error"] = "çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨"
            return status

        # å°è¯•åˆå§‹åŒ–ChromaKnowledgeStore
        try:
            from nanobot.knowledge.rag_config import RAGConfig

            # å…ˆæŸ¥è¯¢çŸ¥è¯†åº“
            config = load_config()
            rag_config = RAGConfig()
            
            # ä»Žconfig.jsonçš„agents.defaultsä¸­è¯»å–RAGé…ç½®
            if hasattr(config.agents, 'defaults'):
                defaults = config.agents.defaults
                if hasattr(defaults, 'embedding_model'):
                    rag_config.embedding_model = defaults.embedding_model
                if hasattr(defaults, 'chunk_size'):
                    rag_config.chunk_size = defaults.chunk_size
                if hasattr(defaults, 'chunk_overlap'):
                    rag_config.chunk_overlap = defaults.chunk_overlap
                if hasattr(defaults, 'top_k'):
                    rag_config.top_k = defaults.top_k
                if hasattr(defaults, 'similarity_threshold'):
                    rag_config.similarity_threshold = defaults.similarity_threshold
                if hasattr(defaults, 'batch_size'):
                    rag_config.batch_size = defaults.batch_size
                if hasattr(defaults, 'timeout'):
                    rag_config.timeout = defaults.timeout
                if hasattr(defaults, 'rerank_model_path'):
                    rag_config.rerank_model_path = defaults.rerank_model_path
                if hasattr(defaults, 'rerank_threshold'):
                    rag_config.rerank_threshold = defaults.rerank_threshold
            
            # å…¼å®¹æ—§çš„reranké…ç½®ä½ç½®
            if config.rerank.model_path:
                rag_config.rerank_model_path = config.rerank.model_path
            if config.rerank.threshold > 0:
                rag_config.rerank_threshold = config.rerank.threshold

            store = ChromaKnowledgeStore(workspace_path, rag_config)
            status["available"] = True

            # èŽ·å–é›†åˆä¿¡æ¯
            collections = store.chroma_client.list_collections()
            status["total_collections"] = len(collections)

            # è®¡ç®—æ€»æ–‡æ¡£æ•°
            total_docs = 0
            for collection in collections:
                try:
                    count = collection.count()
                    total_docs += count
                except:
                    pass
            status["total_documents"] = total_docs

        except Exception as e:
            status["error"] = f"ChromaKnowledgeStoreåˆå§‹åŒ–å¤±è´¥: {str(e)}"

    except ImportError as e:
        status = {
            "available": False,
            "error": f"çŸ¥è¯†åº“æ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}"
        }
    except Exception as e:
        status = {
            "available": False,
            "error": f"çŸ¥è¯†åº“è¯Šæ–­å¤±è´¥: {str(e)}"
        }

    return status


class ConnectionManager:
    """Manage WebSocket connections."""

    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)


# Create FastAPI application
web_app = FastAPI(
    title="nanobot Web UI",
    description="Web interface for nanobot"
)

# Create connection manager instance
manager = ConnectionManager()

# Global instances for provider and agent_loop
provider: LLMProvider = None
agent_loop: AgentLoop = None
config: Config = None


def initialize_webui_resources():
    """Initialize resources for webui."""
    global provider, agent_loop, config
    from nanobot.config.loader import load_config
    from nanobot.bus.queue import MessageBus
    from nanobot.agent.loop import AgentLoop
    from nanobot.providers.litellm_provider import LiteLLMProvider

    bus = MessageBus()

    config = load_config()

    # Create provider from config
    p = config.get_provider()
    model = config.agents.defaults.model
    if not (p and p.api_key) and not model.startswith("bedrock/"):
        return False

    provider = LiteLLMProvider(
        api_key=p.api_key if p else None,
        api_base=config.get_api_base(),
        default_model=model,
        extra_headers=p.extra_headers if p else None,
        provider_name=config.get_provider_name(),
    )

    agent_loop = AgentLoop(
        bus=bus,
        provider=provider,
        workspace=config.workspace_path,
        brave_api_key=config.tools.web.search.api_key or None,
        exec_config=config.tools.exec,
        restrict_to_workspace=config.tools.restrict_to_workspace,
    )

    # è¯Šæ–­çŸ¥è¯†åº“çŠ¶æ€
    knowledge_status = diagnose_knowledge_base(config.workspace_path)
    logger.info(f"[WEB] ðŸ“š çŸ¥è¯†åº“çŠ¶æ€: {knowledge_status}")

    return True


def load_html_template(template_name: str) -> str:
    """Load HTML template from file."""
    template_path = Path(__file__).parent / "templates" / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return f"<html><body><h1>Template not found: {template_name}</h1></body></html>"
    except Exception as e:
        return f"<html><body><h1>Error loading template: {str(e)}</h1></body></html>"


@web_app.get("/")
async def get():
    """Serve the Web UI homepage."""
    html_content = load_html_template("index.html")
    return HTMLResponse(content=html_content)


@web_app.get("/api/knowledge/preview")
async def preview_knowledge_item(item_id: str = None, source_url: str = None, file_path: str = None):
    """Preview knowledge item content."""
    try:
        from nanobot.config.loader import load_config
        from nanobot.knowledge.store import ChromaKnowledgeStore
        from nanobot.knowledge.rag_config import RAGConfig
        import os

        config = load_config()
        rag_config = RAGConfig()
        
        # ä»Žconfig.jsonçš„agents.defaultsä¸­è¯»å–RAGé…ç½®
        if hasattr(config.agents, 'defaults'):
            defaults = config.agents.defaults
            if hasattr(defaults, 'embedding_model'):
                rag_config.embedding_model = defaults.embedding_model
            if hasattr(defaults, 'chunk_size'):
                rag_config.chunk_size = defaults.chunk_size
            if hasattr(defaults, 'chunk_overlap'):
                rag_config.chunk_overlap = defaults.chunk_overlap
            if hasattr(defaults, 'top_k'):
                rag_config.top_k = defaults.top_k
            if hasattr(defaults, 'similarity_threshold'):
                rag_config.similarity_threshold = defaults.similarity_threshold
            if hasattr(defaults, 'batch_size'):
                rag_config.batch_size = defaults.batch_size
            if hasattr(defaults, 'timeout'):
                rag_config.timeout = defaults.timeout
            if hasattr(defaults, 'rerank_model_path'):
                rag_config.rerank_model_path = defaults.rerank_model_path
            if hasattr(defaults, 'rerank_threshold'):
                rag_config.rerank_threshold = defaults.rerank_threshold
        
        # å…¼å®¹æ—§çš„reranké…ç½®ä½ç½®
        if config.rerank.model_path:
            rag_config.rerank_model_path = config.rerank.model_path
        if config.rerank.threshold > 0:
            rag_config.rerank_threshold = config.rerank.threshold

        store = ChromaKnowledgeStore(config.workspace_path, rag_config)

        # æ ¹æ®æä¾›çš„å‚æ•°èŽ·å–æ–‡æ¡£å†…å®¹
        if item_id:
            # é€šè¿‡item_idèŽ·å–çŸ¥è¯†æ¡ç›®çš„å®Œæ•´å†…å®¹
            full_content = await get_full_document_content(store, item_id)
            if full_content:
                return {
                    "status": "success",
                    "message": "æ–‡æ¡£é¢„è§ˆæˆåŠŸ",
                    "item_id": item_id,
                    "content": full_content["content"],
                    "metadata": {
                        "source": "knowledge_base",
                        "title": full_content.get("title", ""),
                        "domain": full_content.get("domain", ""),
                        "category": full_content.get("category", ""),
                        "tags": full_content.get("tags", []),
                        "created_at": full_content.get("created_at", ""),
                        "source_url": full_content.get("source_url", ""),
                        "file_path": full_content.get("file_path", ""),
                        "preview_available": True
                    }
                }
            else:
                return {
                    "status": "error",
                    "message": f"æœªæ‰¾åˆ°IDä¸º {item_id} çš„çŸ¥è¯†æ¡ç›®"
                }

        elif source_url:
            # é€šè¿‡URLèŽ·å–æ–‡æ¡£å†…å®¹
            try:
                # è¿™é‡Œå¯ä»¥å®žçŽ°URLå†…å®¹æŠ“å–ï¼Œæš‚æ—¶è¿”å›žæ¨¡æ‹Ÿå†…å®¹
                return {
                    "status": "success",
                    "message": "URLæ–‡æ¡£é¢„è§ˆæˆåŠŸ",
                    "source_url": source_url,
                    "content": f"URLæ–‡æ¡£å†…å®¹é¢„è§ˆ:\n\næ¥æº: {source_url}\n\næ³¨æ„ï¼šURLå†…å®¹æŠ“å–åŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥å®žçŽ°ï¼Œå½“å‰æ˜¾ç¤ºçš„æ˜¯æ¨¡æ‹Ÿå†…å®¹ã€‚",
                    "metadata": {
                        "source": "url",
                        "preview_available": True
                    }
                }
            except Exception as e:
                return {
                    "status": "error",
                    "message": f"èŽ·å–URLå†…å®¹å¤±è´¥: {str(e)}"
                }

        elif file_path:
            # é€šè¿‡æ–‡ä»¶è·¯å¾„èŽ·å–æ–‡æ¡£å†…å®¹
            try:
                # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨å·¥ä½œç©ºé—´å†…
                workspace_path = str(config.workspace_path)
                abs_file_path = os.path.abspath(file_path)

                if not abs_file_path.startswith(workspace_path):
                    return {
                        "status": "error",
                        "message": "æ–‡ä»¶è·¯å¾„è¶…å‡ºå·¥ä½œç©ºé—´èŒƒå›´ï¼Œæ‹’ç»è®¿é—®"
                    }

                if not os.path.exists(abs_file_path):
                    return {
                        "status": "error",
                        "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                    }

                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(abs_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                return {
                    "status": "success",
                    "message": "æ–‡ä»¶é¢„è§ˆæˆåŠŸ",
                    "file_path": file_path,
                    "content": content,
                    "metadata": {
                        "source": "file",
                        "file_size": os.path.getsize(abs_file_path),
                        "preview_available": True
                    }
                }
            except UnicodeDecodeError:
                return {
                    "status": "error",
                    "message": "æ–‡ä»¶ç¼–ç ä¸æ”¯æŒï¼Œæ— æ³•é¢„è§ˆ"
                }
            except Exception as e:
                return {
                    "status": "error",
                    "message": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"
                }
        else:
            return {
                "status": "error",
                "message": "è¯·æä¾›item_idã€source_urlæˆ–file_pathå‚æ•°"
            }

    except Exception as e:
        return {
            "status": "error",
            "message": f"æ–‡æ¡£é¢„è§ˆå¤±è´¥: {str(e)}"
        }


async def get_full_document_content(store, item_id: str):
    """èŽ·å–çŸ¥è¯†æ¡ç›®çš„å®Œæ•´æ–‡æ¡£å†…å®¹."""
    try:
        # æŸ¥æ‰¾è¯¥çŸ¥è¯†æ¡ç›®æ‰€å±žçš„é¢†åŸŸ
        domain = None
        all_collections = store.chroma_client.list_collections()

        for coll_info in all_collections:
            coll_name = coll_info.name
            if coll_name.startswith("knowledge_"):
                try:
                    collection = store.chroma_client.get_collection(coll_name)
                    # æŸ¥è¯¢è¯¥é›†åˆä¸­æ˜¯å¦æœ‰è¯¥ item_id çš„åˆ†å—
                    results = collection.get(
                        where={"item_id": item_id},
                        include=["documents", "metadatas"]
                    )

                    if results and results["ids"] and len(results["ids"]) > 0:
                        domain = coll_name.replace("knowledge_", "")
                        break
                except Exception as e:
                    logger.warning(f"æŸ¥è¯¢é›†åˆ {coll_name} å¤±è´¥: {str(e)}")
                    continue

        if not domain:
            return None

        # èŽ·å–è¯¥çŸ¥è¯†æ¡ç›®çš„æ‰€æœ‰åˆ†å—
        collection = store.chroma_client.get_collection(f"knowledge_{domain}")
        chunks = collection.get(
            where={"item_id": item_id},
            include=["documents", "metadatas"]
        )

        if not chunks or not chunks["ids"]:
            return None

        # æŒ‰ chunk_index æŽ’åºå¹¶åˆå¹¶å†…å®¹
        chunk_data = []
        metadata = None

        for i in range(len(chunks["ids"])):
            chunk_metadata = chunks["metadatas"][i]
            chunk_document = chunks["documents"][i]
            chunk_index = chunk_metadata.get("chunk_index", 0)

            chunk_data.append({
                "index": chunk_index,
                "text": chunk_document,
                "metadata": chunk_metadata
            })

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ†å—çš„å…ƒæ•°æ®ä½œä¸ºæ•´ä½“å…ƒæ•°æ®
            if metadata is None:
                metadata = chunk_metadata

        # æŒ‰ç´¢å¼•æŽ’åº
        chunk_data.sort(key=lambda x: x["index"])

        # åˆå¹¶æ‰€æœ‰åˆ†å—çš„æ–‡æœ¬
        full_content = " ".join(chunk["text"] for chunk in chunk_data)

        return {
            "content": full_content,
            "title": metadata.get("title", ""),
            "domain": metadata.get("domain", ""),
            "category": metadata.get("category", ""),
            "tags": metadata.get("tags", []),
            "created_at": metadata.get("created_at", ""),
            "updated_at": metadata.get("updated_at", ""),
            "source_url": metadata.get("source_url", ""),
            "file_path": metadata.get("file_path", ""),
            "source": metadata.get("source", ""),
            "priority": metadata.get("priority", 1)
        }

    except Exception as e:
        logger.error(f"èŽ·å–å®Œæ•´æ–‡æ¡£å†…å®¹å¤±è´¥: {str(e)}")
        return None


@web_app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """Handle WebSocket connections with real-time streaming."""
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # Process user message with real-time streaming
            await process_user_message_streaming(data, websocket)
    except WebSocketDisconnect:
        manager.disconnect(websocket)


async def classify_user_intent(user_input: str, websocket: WebSocket) -> str:
    """
    ä½¿ç”¨LLMå¯¹ç”¨æˆ·æ„å›¾è¿›è¡Œåˆ†ç±»
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        websocket: WebSocketè¿žæŽ¥
        
    Returns:
        'A' è¡¨ç¤ºé—®ç­”ç±»ï¼Œ'B' è¡¨ç¤ºæŽ’æŸ¥ç±»
    """
    intent_prompt = f"""åˆ¤æ–­ç”¨æˆ·æ„å›¾ï¼Œä»…å›žå¤ A æˆ– Bã€‚
A: é—®ç­”ç±»ï¼ˆæŸ¥å®šä¹‰ã€æŸ¥é…ç½®ã€é™æ€çŸ¥è¯†ï¼‰
B: æŽ’æŸ¥ç±»ï¼ˆæŠ¥é”™ã€è¿žä¸ä¸Šã€æ£€æŸ¥çŠ¶æ€ã€è¶…æ—¶ï¼ŒæŸ¥é”™, å®šä½, å¯¼è‡´, åŽŸå› , ä¸ºä»€ä¹ˆ, æ€Žä¹ˆåŠž, å¦‚ä½•ï¼‰
C: æŸ¥è¯¢ç±» (æŸ¥podã€æŸ¥ç»„ä»¶ã€æŸ¥çœ‹ã€æŸ¥è¯¢ã€æŸ¥æ—¥å¿—)
é—®é¢˜ï¼š{user_input}"""

    try:
        await websocket.send_text("ðŸ§  æ­£åœ¨è¯†åˆ«ç”¨æˆ·æ„å›¾...\n")

        # ä½¿ç”¨å…¨å±€çš„providerè¿›è¡Œæ„å›¾åˆ†ç±»
        if not provider:
            await websocket.send_text("âš ï¸ LLMæœåŠ¡æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ„å›¾è¯†åˆ«\n")
            return "A"  # é»˜è®¤ä¸ºé—®ç­”ç±»

        # è°ƒç”¨LLMè¿›è¡Œæ„å›¾åˆ†ç±»
        response = await provider.chat(
            messages=[{"role": "user", "content": intent_prompt}],
            model=config.agents.defaults.model,
            max_tokens=10,  # åªéœ€è¦è¿”å›žAæˆ–B
            temperature=0.1  # ä½Žæ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
        )

        intent = response.content.strip().upper()

        # éªŒè¯è¿”å›žç»“æžœ
        if intent not in ['A', 'B', 'C']:
            await websocket.send_text(f"âš ï¸ æ„å›¾è¯†åˆ«ç»“æžœå¼‚å¸¸: {intent}ï¼Œé»˜è®¤ä¸ºé—®ç­”ç±»\n")
            return "A"

        intent_type = "é—®ç­”ç±»"
        if intent == "B":
            intent_type = "æŽ’æŸ¥ç±»"
        if intent == "C":
            intent_type = "æŸ¥è¯¢ç±»"

        await websocket.send_text(f"âœ… ç”¨æˆ·æ„å›¾è¯†åˆ«: {intent_type} ({intent})\n\n")

        return intent

    except Exception as e:
        logger.error(f"æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
        await websocket.send_text(f"âš ï¸ æ„å›¾è¯†åˆ«å¤±è´¥: {str(e)}ï¼Œæ— æ³•å›žç­”\n")
        return "UNKNOWN"  # å‡ºé”™æ—¶é»˜è®¤ä¸ºæœªçŸ¥ç±»


async def process_user_message_streaming(user_input: str, websocket: WebSocket):
    """Process user message with real-time streaming output."""
    import time

    start_time = time.time()

    # Check if provider and agent_loop are initialized
    if not provider or not agent_loop:
        await websocket.send_text("Error: Web UI resources not initialized. Please restart the server.")
        return

    # Send initial processing message
    await websocket.send_text("ðŸ¤– AI Agent is processing your request...\n\n")

    # ç¬¬ä¸€æ­¥ï¼šç”¨æˆ·æ„å›¾è¯†åˆ«
    user_intent = await classify_user_intent(user_input, websocket)

    # æ ¹æ®æ„å›¾å†³å®šå¤„ç†æµç¨‹
    if user_intent == "A":
        # é—®ç­”ç±»ï¼šæŸ¥è¯¢çŸ¥è¯†åº“
        await process_qa_intent(user_input, websocket, start_time)
    elif user_intent == "B" or user_intent == "C":
        # æŽ’æŸ¥ç±»ã€æŸ¥è¯¢ç±»ï¼šç›´æŽ¥è°ƒç”¨LLM
        await process_troubleshooting_intent(user_input, websocket, start_time)


async def process_qa_intent(user_input: str, websocket: WebSocket, start_time: float):
    """å¤„ç†é—®ç­”ç±»æ„å›¾ï¼šä¼˜å…ˆæŸ¥è¯¢çŸ¥è¯†åº“"""
    import time
    import json
    from nanobot.config.loader import load_config
    from nanobot.knowledge.store import ChromaKnowledgeStore

    try:
        config = load_config()

        # åˆ›å»º RAGConfig å¹¶ä»Žé…ç½®æ–‡ä»¶åŠ è½½å®Œæ•´é…ç½®
        from nanobot.knowledge.rag_config import RAGConfig
        rag_config = RAGConfig()
        
        # ä»Žconfig.jsonçš„agents.defaultsä¸­è¯»å–RAGé…ç½®
        if hasattr(config.agents, 'defaults'):
            defaults = config.agents.defaults
            if hasattr(defaults, 'embedding_model'):
                rag_config.embedding_model = defaults.embedding_model
            if hasattr(defaults, 'chunk_size'):
                rag_config.chunk_size = defaults.chunk_size
            if hasattr(defaults, 'chunk_overlap'):
                rag_config.chunk_overlap = defaults.chunk_overlap
            if hasattr(defaults, 'top_k'):
                rag_config.top_k = defaults.top_k
            if hasattr(defaults, 'similarity_threshold'):
                rag_config.similarity_threshold = defaults.similarity_threshold
            if hasattr(defaults, 'batch_size'):
                rag_config.batch_size = defaults.batch_size
            if hasattr(defaults, 'timeout'):
                rag_config.timeout = defaults.timeout
            if hasattr(defaults, 'rerank_model_path'):
                rag_config.rerank_model_path = defaults.rerank_model_path
            if hasattr(defaults, 'rerank_threshold'):
                rag_config.rerank_threshold = defaults.rerank_threshold
        
        # å…¼å®¹æ—§çš„reranké…ç½®ä½ç½®
        if config.rerank.model_path:
            rag_config.rerank_model_path = config.rerank.model_path
        if config.rerank.threshold > 0:
            rag_config.rerank_threshold = config.rerank.threshold

        store = ChromaKnowledgeStore(config.workspace_path, rag_config)
    except RuntimeError as e:
        # CrossEncoder åˆå§‹åŒ–å¤±è´¥
        error_msg = f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}\n\næœåŠ¡å¯åŠ¨ç»ˆæ­¢ï¼Œè¯·æ£€æŸ¥ CrossEncoder æ¨¡åž‹é…ç½®ã€‚\n"
        await websocket.send_text(error_msg)
        # å…³é—­WebSocketè¿žæŽ¥
        await websocket.close(code=1011, reason="CrossEncoder initialization failed")
        return
    except Exception as e:
        # å…¶ä»–åˆå§‹åŒ–é”™è¯¯
        error_msg = f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}\n\n"
        await websocket.send_text(error_msg)
        return

    # å‘é€çŸ¥è¯†åº“æŸ¥è¯¢å¼€å§‹ä¿¡æ¯
    await websocket.send_text("ðŸ“š æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†åº“...\n")

    # æœç´¢çŸ¥è¯†åº“ï¼Œè¿”å›žå¾—åˆ†
    search_result = store.search_knowledge(query=user_input, return_scores=True)

    # æ£€æŸ¥è¿”å›žå€¼ç±»åž‹
    if isinstance(search_result, tuple) and len(search_result) == 2:
        knowledge_results, scores = search_result
    else:
        knowledge_results = search_result
        scores = []

    # é—®ç­”ç±»å¤„ç†ï¼šæœ‰ç»“æžœå°±è¿”å›žï¼Œæ²¡ç»“æžœå›žç­”"ä¸çŸ¥é“"
    if knowledge_results and scores:
        # èŽ·å–é‡æŽ’åºå¾—åˆ†æœ€é«˜çš„ç»“æžœ
        top_score = scores[0].get('rerank_score', 0)

        await websocket.send_text(f"âœ… çŸ¥è¯†åº“æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(knowledge_results)} ä¸ªç»“æžœ\n")
        await websocket.send_text(f"ðŸ“Š æœ€é«˜é‡æŽ’åºå¾—åˆ†: {top_score:.2f}\n\n")

        # æ ¼å¼åŒ–çŸ¥è¯†åº“ç»“æžœï¼ŒåŒ…å«é¢„è§ˆä¿¡æ¯
        top_item = knowledge_results[0]

        # æ·»åŠ é¢„è§ˆä¿¡æ¯
        preview_links = []

        # æ£€æŸ¥æ–‡æ¡£é“¾æŽ¥
        if hasattr(top_item, 'source_url') and top_item.source_url:
            preview_links.append(f"ðŸ“„ æ–‡æ¡£é“¾æŽ¥: {top_item.source_url}")

        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        if hasattr(top_item, 'file_path') and top_item.file_path:
            preview_links.append(f"ðŸ“ æ–‡ä»¶è·¯å¾„: {top_item.file_path}")

        # æ£€æŸ¥æ˜¯å¦å¯é¢„è§ˆ
        if hasattr(top_item, 'preview_available') and top_item.preview_available:
            preview_links.append("ðŸ” æ”¯æŒé¢„è§ˆ")

        # æ·»åŠ çŸ¥è¯†æ¡ç›®IDç”¨äºŽé¢„è§ˆ
        if hasattr(top_item, 'id') and top_item.id:
            preview_links.append(f"ðŸ†” æ¡ç›®ID: {top_item.id}")

        preview_info = ""
        if preview_links:
            preview_info = f"\n**é¢„è§ˆä¿¡æ¯**: {' | '.join(preview_links)}"

        # æ ¼å¼åŒ–çŸ¥è¯†åº“ç»“æžœ
        formatted_result = f"""### 1. {top_item.title}
**Domain**: {top_item.domain} | **Category**: {top_item.category} | **Priority**: {top_item.priority}
**Tags**: {', '.join(top_item.tags)}
**Created**: {top_item.created_at[:10]}{preview_info}

{top_item.content}

---
"""

        # æž„å»ºé¢„è§ˆé¡¹ç›®æ•°ç»„ï¼ˆåŽ»é‡é€»è¾‘ï¼šç›¸åŒæ–‡ä»¶åªæ˜¾ç¤ºä¸€ä¸ªé¢„è§ˆæŒ‰é’®ï¼‰
        preview_items = []
        seen_files = set()  # ç”¨äºŽåŽ»é‡

        # ä¼˜å…ˆçº§1ï¼šæ–‡ä»¶è·¯å¾„é¢„è§ˆï¼ˆå¦‚æžœæœ‰æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼‰
        if hasattr(top_item, 'file_path') and top_item.file_path:
            file_key = top_item.file_path
            if file_key not in seen_files:
                preview_items.append({
                    'type': 'file',
                    'id': top_item.file_path,
                    'label': 'ðŸ“ é¢„è§ˆæ–‡ä»¶å†…å®¹',
                    'path': top_item.file_path
                })
                seen_files.add(file_key)

        # ä¼˜å…ˆçº§2ï¼šæ–‡æ¡£é“¾æŽ¥é¢„è§ˆï¼ˆå¦‚æžœæ²¡æœ‰æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œä½†æœ‰URLï¼‰
        elif hasattr(top_item, 'source_url') and top_item.source_url:
            url_key = top_item.source_url
            if url_key not in seen_files:
                preview_items.append({
                    'type': 'url',
                    'id': top_item.source_url,
                    'label': 'ðŸ“„ é¢„è§ˆæ–‡æ¡£é“¾æŽ¥',
                    'url': top_item.source_url
                })
                seen_files.add(url_key)

        # ä¼˜å…ˆçº§3ï¼šçŸ¥è¯†æ¡ç›®å†…å®¹é¢„è§ˆï¼ˆå¦‚æžœæ—¢æ²¡æœ‰æ–‡ä»¶è·¯å¾„ä¹Ÿæ²¡æœ‰URLï¼Œä½†å¯é¢„è§ˆï¼‰
        elif hasattr(top_item, 'id') and top_item.id and hasattr(top_item,
                                                                 'preview_available') and top_item.preview_available:
            item_key = f"item_{top_item.id}"
            if item_key not in seen_files:
                preview_items.append({
                    'type': 'item',
                    'id': top_item.id,
                    'label': 'ðŸ” é¢„è§ˆå®Œæ•´å†…å®¹',
                    'item_id': top_item.id
                })
                seen_files.add(item_key)

        # é€šè¿‡JSONæ ¼å¼å‘é€çŸ¥è¯†åº“ç»“æžœï¼Œè¿™æ ·å‰ç«¯å¯ä»¥è§£æžé¢„è§ˆä¿¡æ¯
        knowledge_message = {
            'type': 'stream_chunk',
            'content_type': 'knowledge',
            'content': f"æ‰¾åˆ° {len(knowledge_results)} ä¸ªç»“æžœï¼Œæœ€é«˜å¾—åˆ†: {top_score:.2f}",
            'knowledge_status': 'success',
            'knowledge_count': len(knowledge_results),
            'knowledge_result': formatted_result,
            'preview_items': preview_items,  # æ–°å¢žé¢„è§ˆé¡¹ç›®æ•°ç»„
            'timestamp': time.time(),
            'duration_from_start': round(time.time() - start_time, 3)
        }

        await websocket.send_text(json.dumps(knowledge_message, ensure_ascii=False))

        # é—®ç­”ç±»ï¼šç›´æŽ¥è¾“å‡ºçŸ¥è¯†åº“ç»“æžœï¼Œä¸å†è°ƒç”¨LLM
        await websocket.send_text("ðŸ“š çŸ¥è¯†åº“ç­”æ¡ˆï¼š\n")
        await websocket.send_text(f"{knowledge_results[0].content}\n\n")

        # å‘é€å¤„ç†æ—¶é—´
        end_time = time.time()
        total_processing_time = round(end_time - start_time, 1)
        await websocket.send_text(f"\n---\n*æ€»è€—æ—¶: {total_processing_time}ç§’*\n")
        return
    else:
        # é—®ç­”ç±»ï¼šæ²¡æœ‰æ‰¾åˆ°çŸ¥è¯†åº“ç»“æžœï¼Œå›žç­”"ä¸çŸ¥é“"
        await websocket.send_text("ðŸ“­ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æžœ\n\n")
        await websocket.send_text("ðŸ¤– æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ— æ³•å›žç­”æ‚¨çš„é—®é¢˜ã€‚\n\n")

        # å‘é€å¤„ç†æ—¶é—´
        end_time = time.time()
        total_processing_time = round(end_time - start_time, 1)
        await websocket.send_text(f"\n---\n*æ€»è€—æ—¶: {total_processing_time}ç§’*\n")
        return


async def process_troubleshooting_intent(user_input: str, websocket: WebSocket, start_time: float):
    """å¤„ç†æŽ’æŸ¥ç±»æ„å›¾ï¼šç›´æŽ¥è°ƒç”¨LLM"""
    import time
    import json

    await websocket.send_text("ðŸ”§ æ£€æµ‹åˆ°æŽ’æŸ¥ç±»é—®é¢˜ï¼Œç›´æŽ¥è°ƒç”¨AIåˆ†æž...\n\n")

    # Record LLM start time
    llm_start_time = time.time()

    # è®¾ç½®æµå¼å›žè°ƒå‡½æ•°
    async def stream_callback(context_info: dict):
        """æµå¼è¾“å‡ºå›žè°ƒå‡½æ•°ï¼ŒæŒ‰ç±»åž‹åˆ†ç±»æ˜¾ç¤ºå†…å®¹ï¼Œå¹¶ç»Ÿè®¡æ¯æ¬¡è¿”å›žçš„è€—æ—¶"""
        content = context_info.get('content', '')

        # èŽ·å–å›žè°ƒæ—¶é—´å’Œè®¡ç®—è€—æ—¶
        callback_time = time.time()
        current_duration = round(callback_time - start_time, 3)

        # èŽ·å–è¿­ä»£è®¡æ•°
        iteration_count = context_info.get('iteration_count', 0)

        # æ ¹æ®å†…å®¹ç±»åž‹è¿›è¡Œåˆ†ç±»å¤„ç†
        content_type = 'text'  # é»˜è®¤ä¸ºæ–‡æœ¬ç±»åž‹
        enhanced_content = content

        # æ£€æµ‹æ˜¯å¦ä¸ºå·¥å…·è°ƒç”¨
        if context_info.get('is_tool_call') or 'tool_name' in context_info:
            content_type = 'tool'
            tool_name = context_info.get('tool_name', '')
            tool_status = context_info.get('tool_status', '')

            if tool_status == 'start':
                enhanced_content = f"ðŸ”§ è°ƒç”¨å·¥å…·: {tool_name}"
            elif tool_status == 'success':
                enhanced_content = f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_name}"
            elif tool_status == 'error':
                enhanced_content = f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_name}"
            else:
                enhanced_content = content

        # æ£€æµ‹æ˜¯å¦ä¸ºæŽ¨ç†è¿‡ç¨‹
        elif context_info.get('is_reasoning'):
            content_type = 'reasoning'
            enhanced_content = f"ðŸ¤” {content}"

        # æ£€æµ‹æ˜¯å¦ä¸ºçŸ¥è¯†åº“æŸ¥è¯¢
        elif context_info.get('is_knowledge_query'):
            content_type = 'knowledge'
            enhanced_content = f"ðŸ“š {content}"

        # æ£€æµ‹æ˜¯å¦ä¸ºæœ€ç»ˆç­”æ¡ˆ
        elif context_info.get('is_final_answer'):
            content_type = 'final_answer'
            enhanced_content = f"ðŸ’¡ {content}"

        # æ£€æµ‹æ˜¯å¦ä¸ºè¿­ä»£å¼€å§‹
        elif context_info.get('is_iteration_start'):
            content_type = 'iteration'
            enhanced_content = f"ðŸ”„ ç¬¬ {iteration_count} è½®æ€è€ƒ: {content}"

        # æž„å»ºæ¶ˆæ¯æ•°æ®
        message_data = {
            'type': 'stream_chunk',
            'content_type': content_type,
            'content': enhanced_content,
            'is_reasoning': context_info.get('is_reasoning', False),
            'is_tool_call': content_type == 'tool' or context_info.get('is_tool_call', False),
            'is_final_answer': context_info.get('is_final_answer', False),
            'is_iteration_start': context_info.get('is_iteration_start', False),
            'timestamp': callback_time,
            'duration_from_start': current_duration,
            'iteration_count': iteration_count,
        }

        # å¦‚æžœæ˜¯å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ å·¥å…·åç§°å’ŒçŠ¶æ€ä¿¡æ¯
        if content_type == 'tool':
            message_data['tool_name'] = context_info.get('tool_name', '')
            message_data['tool_status'] = context_info.get('tool_status', '')
            message_data['tool_duration'] = context_info.get('tool_duration', 0)
            message_data['tool_result'] = context_info.get('tool_result', '')
            message_data['tool_error'] = context_info.get('tool_error', '')
            message_data['tool_args'] = context_info.get('tool_args')

        # å¦‚æžœæ˜¯çŸ¥è¯†åº“æŸ¥è¯¢ï¼Œæ·»åŠ çŸ¥è¯†åº“ç›¸å…³ä¿¡æ¯
        if content_type == 'knowledge':
            message_data['knowledge_status'] = context_info.get('knowledge_status', '')
            message_data['knowledge_domain'] = context_info.get('knowledge_domain', '')
            message_data['knowledge_query'] = context_info.get('knowledge_query', '')
            message_data['knowledge_count'] = context_info.get('knowledge_count', 0)
            message_data['knowledge_result'] = context_info.get('knowledge_result', '')

        await websocket.send_text(json.dumps(message_data, ensure_ascii=False))

    # ä¸ºagent_loopè®¾ç½®æµå¼å›žè°ƒ
    agent_loop.stream_callback = stream_callback

    # Process with streaming output
    response = await agent_loop.process_direct(user_input, session_key="cli:webui")

    # Record LLM end time
    llm_end_time = time.time()
    llm_execution_time = round(llm_end_time - llm_start_time, 1)

    # Send the actual response (å¦‚æžœæµå¼è¾“å‡ºå·²ç»å‘é€äº†å†…å®¹ï¼Œè¿™é‡Œå¯èƒ½ä¸éœ€è¦å†å‘é€)
    if response and response.strip():
        # æ£€æŸ¥æ˜¯å¦å·²ç»é€šè¿‡æµå¼è¾“å‡ºå‘é€äº†å†…å®¹
        # å¦‚æžœæ²¡æœ‰æµå¼è¾“å‡ºï¼Œåˆ™å‘é€å®Œæ•´å“åº”
        await websocket.send_text("\n" + response)
    elif not response:
        await websocket.send_text("No response from agent.")

    end_time = time.time()
    total_processing_time = round(end_time - start_time, 1)

    # Send processing times
    await websocket.send_text(f"\n---\n*æ€»è€—æ—¶: {total_processing_time}ç§’ | LLMæ‰§è¡Œè€—æ—¶: {llm_execution_time}ç§’*")


async def process_user_message(user_input: str) -> str:
    """Process user message using nanobot's AgentLoop."""
    import time

    start_time = time.time()

    # Check if provider and agent_loop are initialized
    if not provider or not agent_loop:
        return "Error: Web UI resources not initialized. Please restart the server."

    # Record LLM start time
    llm_start_time = time.time()

    response = await agent_loop.process_direct(user_input, session_key="cli:webui")

    # Record LLM end time
    llm_end_time = time.time()
    llm_execution_time = round(llm_end_time - llm_start_time, 1)

    end_time = time.time()
    total_processing_time = round(end_time - start_time, 1)

    if response:
        return f"{response}\n\n---\n*æ€»è€—æ—¶: {total_processing_time}ç§’ | LLMæ‰§è¡Œè€—æ—¶: {llm_execution_time}ç§’*"
    else:
        return f"No response from agent.\n\n---\n*æ€»è€—æ—¶: {total_processing_time}ç§’ | LLMæ‰§è¡Œè€—æ—¶: {llm_execution_time}ç§’*"


@web_app.post("/api/chat")
async def chat_endpoint(message: dict):
    """Handle chat API requests."""
    user_input = message.get("message", "")
    if not user_input:
        return {"error": "No message provided"}

    response = await process_user_message(user_input)
    return {"response": response}
